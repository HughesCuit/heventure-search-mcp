#!/usr/bin/env python3
"""
MCP Web Search Server åŸºå‡†æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨
"""

import json
import datetime
import os
from typing import Dict, Any, List

class BenchmarkReportGenerator:
    """åŸºå‡†æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, config_file: str = "benchmark_config.json"):
        self.config = self.load_config(config_file)
        self.thresholds = self.config.get("performance_thresholds", {})
    
    def load_config(self, config_file: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"é…ç½®æ–‡ä»¶ {config_file} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self.get_default_config()
    
    def get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "performance_thresholds": {
                "search_response_time": {"excellent": 1500, "good": 3000, "acceptable": 5000},
                "content_response_time": {"excellent": 2000, "good": 5000, "acceptable": 10000},
                "success_rate": {"excellent": 90, "good": 75, "acceptable": 60}
            }
        }
    
    def evaluate_performance(self, metric_value: float, metric_type: str) -> tuple[str, str]:
        """è¯„ä¼°æ€§èƒ½æŒ‡æ ‡"""
        thresholds = self.thresholds.get(metric_type, {})
        
        if metric_type.endswith('_rate'):
            # æˆåŠŸç‡ç±»æŒ‡æ ‡ï¼Œå€¼è¶Šé«˜è¶Šå¥½
            if metric_value >= thresholds.get('excellent', 90):
                return "ä¼˜ç§€", "ğŸŸ¢"
            elif metric_value >= thresholds.get('good', 75):
                return "è‰¯å¥½", "ğŸŸ¡"
            elif metric_value >= thresholds.get('acceptable', 60):
                return "å¯æ¥å—", "ğŸŸ "
            else:
                return "éœ€è¦æ”¹è¿›", "ğŸ”´"
        else:
            # å“åº”æ—¶é—´ç±»æŒ‡æ ‡ï¼Œå€¼è¶Šä½è¶Šå¥½
            if metric_value <= thresholds.get('excellent', 2000):
                return "ä¼˜ç§€", "ğŸŸ¢"
            elif metric_value <= thresholds.get('good', 5000):
                return "è‰¯å¥½", "ğŸŸ¡"
            elif metric_value <= thresholds.get('acceptable', 10000):
                return "å¯æ¥å—", "ğŸŸ "
            else:
                return "éœ€è¦æ”¹è¿›", "ğŸ”´"
    
    def generate_markdown_report(self, results: List[Dict[str, Any]], output_file: str = "benchmark_report.md"):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        
        report_lines = []
        
        # æŠ¥å‘Šå¤´éƒ¨
        report_lines.extend([
            "# MCP Web Search Server æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š",
            "",
            f"**ç”Ÿæˆæ—¶é—´**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**æµ‹è¯•ç¯å¢ƒ**: {os.uname().sysname} {os.uname().release}",
            "",
            "## ğŸ“Š æµ‹è¯•æ¦‚è§ˆ",
            ""
        ])
        
        # æµ‹è¯•ç»“æœæ¦‚è§ˆè¡¨æ ¼
        report_lines.extend([
            "| æµ‹è¯•ç±»å‹ | æ€»è¯·æ±‚æ•° | æˆåŠŸè¯·æ±‚æ•° | æˆåŠŸç‡ | å¹³å‡å“åº”æ—¶é—´ | æ€§èƒ½è¯„çº§ |",
            "|----------|----------|------------|--------|--------------|----------|"
        ])
        
        for result in results:
            test_type = result.get('type', 'Unknown')
            total_requests = result.get('total_requests', result.get('concurrent_requests', 'N/A'))
            successful_requests = result.get('successful_requests', 'N/A')
            success_rate = result.get('success_rate', 'N/A')
            avg_response_time = result.get('avg_response_time', 'N/A')
            
            # è¯„ä¼°æ€§èƒ½
            if success_rate != 'N/A':
                success_rate_value = float(success_rate.replace('%', ''))
                _, success_icon = self.evaluate_performance(success_rate_value, 'success_rate')
            else:
                success_icon = "â“"
            
            if avg_response_time != 'N/A':
                response_time_value = float(avg_response_time.replace('ms', ''))
                if 'search' in test_type.lower() or 'æœç´¢' in test_type:
                    _, time_icon = self.evaluate_performance(response_time_value, 'search_response_time')
                else:
                    _, time_icon = self.evaluate_performance(response_time_value, 'content_response_time')
            else:
                time_icon = "â“"
            
            performance_rating = f"{success_icon}{time_icon}"
            
            report_lines.append(
                f"| {test_type} | {total_requests} | {successful_requests} | {success_rate} | {avg_response_time} | {performance_rating} |"
            )
        
        report_lines.extend([
            "",
            "## ğŸ“ˆ è¯¦ç»†åˆ†æ",
            ""
        ])
        
        # è¯¦ç»†åˆ†ææ¯ä¸ªæµ‹è¯•ç±»å‹
        for result in results:
            test_type = result.get('type', 'Unknown')
            report_lines.extend([
                f"### {test_type}",
                ""
            ])
            
            # åŸºæœ¬æŒ‡æ ‡
            report_lines.append("**åŸºæœ¬æŒ‡æ ‡:**")
            report_lines.append("")
            
            for key, value in result.items():
                if key != 'type':
                    # æ ¼å¼åŒ–é”®å
                    key_cn = {
                        'total_requests': 'æ€»è¯·æ±‚æ•°',
                        'successful_requests': 'æˆåŠŸè¯·æ±‚æ•°',
                        'success_rate': 'æˆåŠŸç‡',
                        'avg_response_time': 'å¹³å‡å“åº”æ—¶é—´',
                        'min_response_time': 'æœ€å°å“åº”æ—¶é—´',
                        'max_response_time': 'æœ€å¤§å“åº”æ—¶é—´',
                        'median_response_time': 'ä¸­ä½æ•°å“åº”æ—¶é—´',
                        'concurrent_requests': 'å¹¶å‘è¯·æ±‚æ•°',
                        'total_time': 'æ€»è€—æ—¶',
                        'throughput': 'ååé‡'
                    }.get(key, key)
                    
                    report_lines.append(f"- **{key_cn}**: {value}")
            
            report_lines.append("")
            
            # æ€§èƒ½è¯„ä¼°
            if 'success_rate' in result and 'avg_response_time' in result:
                success_rate_value = float(result['success_rate'].replace('%', ''))
                response_time_value = float(result['avg_response_time'].replace('ms', ''))
                
                success_eval, success_icon = self.evaluate_performance(success_rate_value, 'success_rate')
                
                if 'search' in test_type.lower() or 'æœç´¢' in test_type:
                    time_eval, time_icon = self.evaluate_performance(response_time_value, 'search_response_time')
                else:
                    time_eval, time_icon = self.evaluate_performance(response_time_value, 'content_response_time')
                
                report_lines.extend([
                    "**æ€§èƒ½è¯„ä¼°:**",
                    "",
                    f"- æˆåŠŸç‡: {success_icon} {success_eval}",
                    f"- å“åº”æ—¶é—´: {time_icon} {time_eval}",
                    ""
                ])
        
        # æ€»ä½“å»ºè®®
        report_lines.extend([
            "## ğŸ’¡ ä¼˜åŒ–å»ºè®®",
            "",
            self.generate_recommendations(results),
            "",
            "## ğŸ”§ æ€§èƒ½è°ƒä¼˜æŒ‡å—",
            "",
            "### ç½‘ç»œä¼˜åŒ–",
            "- ä½¿ç”¨CDNåŠ é€Ÿé™æ€èµ„æº",
            "- å¯ç”¨HTTP/2åè®®",
            "- ä¼˜åŒ–DNSè§£æ",
            "- ä½¿ç”¨è¿æ¥æ± å¤ç”¨è¿æ¥",
            "",
            "### åº”ç”¨ä¼˜åŒ–",
            "- å®ç°æ™ºèƒ½ç¼“å­˜ç­–ç•¥",
            "- ä¼˜åŒ–æœç´¢ç®—æ³•",
            "- ä½¿ç”¨å¼‚æ­¥å¤„ç†",
            "- é™åˆ¶å¹¶å‘è¯·æ±‚æ•°é‡",
            "",
            "### ç›‘æ§å»ºè®®",
            "- è®¾ç½®æ€§èƒ½ç›‘æ§å‘Šè­¦",
            "- å®šæœŸè¿è¡ŒåŸºå‡†æµ‹è¯•",
            "- ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨",
            "- è®°å½•è¯¦ç»†çš„æ€§èƒ½æ—¥å¿—",
            "",
            "---",
            f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*"
        ])
        
        # å†™å…¥æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        print(f"ğŸ“„ MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
    
    def generate_recommendations(self, results: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        for result in results:
            test_type = result.get('type', '')
            
            if 'success_rate' in result:
                success_rate = float(result['success_rate'].replace('%', ''))
                if success_rate < 80:
                    recommendations.append(f"- **{test_type}**: æˆåŠŸç‡è¾ƒä½({result['success_rate']})ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œç›®æ ‡ç½‘ç«™å¯ç”¨æ€§")
            
            if 'avg_response_time' in result:
                response_time = float(result['avg_response_time'].replace('ms', ''))
                if response_time > 5000:
                    recommendations.append(f"- **{test_type}**: å“åº”æ—¶é—´è¾ƒé•¿({result['avg_response_time']})ï¼Œå»ºè®®ä¼˜åŒ–ç½‘ç»œé…ç½®æˆ–å¢åŠ ç¼“å­˜")
                elif response_time > 3000:
                    recommendations.append(f"- **{test_type}**: å“åº”æ—¶é—´åé«˜({result['avg_response_time']})ï¼Œå¯è€ƒè™‘æ€§èƒ½ä¼˜åŒ–")
            
            if 'throughput' in result:
                throughput = float(result['throughput'].split()[0])
                if throughput < 1:
                    recommendations.append(f"- **{test_type}**: ååé‡è¾ƒä½({result['throughput']})ï¼Œå»ºè®®ä¼˜åŒ–å¹¶å‘å¤„ç†èƒ½åŠ›")
        
        if not recommendations:
            recommendations.append("- ğŸ‰ æ‰€æœ‰æµ‹è¯•æŒ‡æ ‡è¡¨ç°è‰¯å¥½ï¼Œç»§ç»­ä¿æŒï¼")
            recommendations.append("- å»ºè®®å®šæœŸè¿è¡ŒåŸºå‡†æµ‹è¯•ä»¥ç›‘æ§æ€§èƒ½å˜åŒ–")
            recommendations.append("- å¯ä»¥è€ƒè™‘åœ¨æ›´é«˜è´Ÿè½½ä¸‹è¿›è¡Œå‹åŠ›æµ‹è¯•")
        
        return '\n'.join(recommendations)
    
    def generate_csv_report(self, results: List[Dict[str, Any]], output_file: str = "benchmark_results.csv"):
        """ç”ŸæˆCSVæ ¼å¼çš„æŠ¥å‘Š"""
        import csv
        
        if not results:
            print("æ²¡æœ‰æµ‹è¯•ç»“æœæ•°æ®")
            return
        
        # è·å–æ‰€æœ‰å¯èƒ½çš„å­—æ®µ
        all_fields = set()
        for result in results:
            all_fields.update(result.keys())
        
        all_fields = sorted(list(all_fields))
        
        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=all_fields)
            writer.writeheader()
            
            for result in results:
                writer.writerow(result)
        
        print(f"ğŸ“Š CSVæŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“‹ MCP Web Search Server åŸºå‡†æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨")
    print("=" * 50)
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æµ‹è¯•ç»“æœæ–‡ä»¶
    result_files = [
        "quick_benchmark_results.json",
        "benchmark_results.json"
    ]
    
    results_data = None
    used_file = None
    
    for file_name in result_files:
        if os.path.exists(file_name):
            try:
                with open(file_name, 'r', encoding='utf-8') as f:
                    results_data = json.load(f)
                used_file = file_name
                print(f"âœ… æ‰¾åˆ°æµ‹è¯•ç»“æœæ–‡ä»¶: {file_name}")
                break
            except Exception as e:
                print(f"âŒ è¯»å–æ–‡ä»¶ {file_name} å¤±è´¥: {e}")
    
    if not results_data:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æµ‹è¯•ç»“æœæ–‡ä»¶")
        print("è¯·å…ˆè¿è¡ŒåŸºå‡†æµ‹è¯•:")
        print("  python quick_benchmark.py")
        print("  æˆ–")
        print("  python benchmark.py")
        return
    
    # ç”ŸæˆæŠ¥å‘Š
    generator = BenchmarkReportGenerator()
    
    print(f"\nğŸ“Š åŸºäº {used_file} ç”ŸæˆæŠ¥å‘Š...")
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    generator.generate_markdown_report(results_data, "benchmark_report.md")
    
    # ç”ŸæˆCSVæŠ¥å‘Š
    generator.generate_csv_report(results_data, "benchmark_results.csv")
    
    print("\nâœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    print("  ğŸ“„ benchmark_report.md - è¯¦ç»†çš„MarkdownæŠ¥å‘Š")
    print("  ğŸ“Š benchmark_results.csv - CSVæ ¼å¼çš„æ•°æ®")
    
if __name__ == "__main__":
    main()